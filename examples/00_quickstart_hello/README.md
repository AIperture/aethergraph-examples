# Quickstart Â· Hello World + `graph_fn` & `NodeContext` intro

> Run an agent in under 90 seconds, understand what each line does, then jump straight to context methods.

---

## TL;DR â€” Run it

```bash
python examples/00_quickstart_hello/run.py
```

**Expected output**

* `AetherGraph sidecar server started at: http://127.0.0.1:...`
* Two channel messages (hello + LLM reply)
* Final print: `Result: {'final_output': 'HELLO WORLD'}`

---

## What this example shows

* **Start the sidecar**: `start()` â€” brings up default services (channel, LLM, memory, logging) so you donâ€™t wire anything manually.
* **Turn a function into an agent**: `@graph_fn` â€” a normal async Python function becomes a firstâ€‘class agent step.
* **Use builtâ€‘in services via context**: `context.channel()`, `context.llm("default")`, `context.logger()` â€” consistent API across local/remote providers.
* **Return structured outputs**: return a `dict` so steps compose cleanly.
* **Run conveniently**: `run(my_fn, inputs={...})` â€” a helper that executes the step with a fresh context.

---

## Code anatomy (lineâ€‘byâ€‘line mental model)

```python
from aethergraph import graph_fn, NodeContext, run
from aethergraph.server import start

url = start()               # 1) Boot the sidecar (default channel/LLM/memory/logger)
print("AetherGraph sidecar server started at:", url)

@graph_fn(name="hello_world") # 2) Decorate a normal async function â†’ agent step
async def hello_world(input_text: str, *, context: NodeContext):
    context.logger().info("hello_world started")         # 3) Structured logs

    await context.channel().send_text(                   # 4) Channel = user I/O
        f"ðŸ‘‹ Hello! You sent: {input_text}")

    llm_text, _usage = await context.llm("default").chat( # 5) LLM access by name
        messages=[
            {"role": "system", "content": "Be brief."},
            {"role": "user", "content": f"Say hi back to: {input_text}"},
        ]
    )
    await context.channel().send_text(f"LLM replied: {llm_text}")

    output = input_text.upper()                          # 6) Your own logic
    context.logger().info("hello_world finished")
    return {"final_output": output}                     # 7) Always return a dict

result = run(hello_world, inputs={"input_text": "hello world"}) # 8) Oneâ€‘liner runner
print("Result:", result)
```

**Why it matters**

* You write ordinary Python; AetherGraph provides the **ambient runtime** (channel/LLM/memory/logging) through `NodeContext`.
* The return `dict` keeps composition explicit (great for fanâ€‘in/fanâ€‘out later).

---

## `graph_fn` in one minute

* **What it is**: a decorator that turns an async function into an **agent step**.
* **Signature**: your parameters are your inputs; you also receive `context: NodeContext`.
* **Contract**: return a `dict` of named outputs.
* **Why**: easy to test, chain, and visualize; no hidden globals.

> Think: *â€œa function with superpowersâ€* â€” it runs with a consistent service set wherever you execute it.

---

## `NodeContext` (essentials youâ€™ll use immediately)

`NodeContext` is your **service hub**. In this example you used:

| Method              | What it gives you                        | Core calls (keep it small)                                             |
| ------------------- | ---------------------------------------- | ---------------------------------------------------------------------- |
| `context.channel()` | User I/O (console/Slack/GUI via sidecar) | `send_text(text)`, `ask_text(prompt)`, `ask_approval(prompt, options)` |
| `context.llm(name)` | An LLM client by name                    | `chat(messages) -> (text, usage)`                                      |
| `context.logger()`  | Structured logging                       | `info(msg)`, `warning(msg)`, `error(msg)`                              |
| `context.memory()`  | Lightweight recent records               | `record(kind, value)`, `recent(kinds=[...])`                           |

> You donâ€™t need to configure providers here. The sidecar supplies sensible defaults; later you can swap in Slack/Telegram or your preferred LLM.

---

## Next: minimal **Context Methods** tour

Move on to short, runnable snippets that focus on one method at a time:

* `examples/02_context_methods_min/01_channel_send_text.py` â€” send a message
* `examples/02_context_methods_min/02_channel_ask_approval.py` â€” humanâ€‘inâ€‘theâ€‘loop
* `examples/02_context_methods_min/10_llm_chat.py` â€” singleâ€‘turn chat
* `examples/02_context_methods_min/20_artifacts_put_json.py` / `21_artifacts_get_json.py` â€” (if you enable artifacts)
* `examples/02_context_methods_min/30_kv_set_get.py` â€” ephemeral state
* `examples/02_context_methods_min/40_mem_lifecycle.py` â€” lifecycle breadcrumbs

Each file is â‰¤12 lines and runs offline with the inâ€‘memory defaults.

---

## Troubleshooting

* **No sidecar URL printed?** Ensure `start()` is called before running your step.
* **LLM reply looks like an echo/stub?** Thatâ€™s expected in dev; flip your sidecar to a real provider later.
* **Interactive prompts block CI?** Comment out `ask_text/ask_approval` in quickstart; keep them for the context tour.

---

## Why this layout scales

* New users succeed in minutes with zero credentials.
* The same code runs on console or Slack by swapping sidecar config â€” **no code changes**.
* The mental model (function â†’ agent, `context.*` services, dict outputs) stays the same as you add graphifying, tools, and bigger demos.
